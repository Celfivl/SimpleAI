import os, sys, argparse, json, tiktoken
from dotenv import load_dotenv
from functions.get_files_info import schema_get_files_info, schema_get_file_content, schema_run_python_file, schema_write_file
from functions.get_files_info import get_files_info, get_file_content, write_file
from functions.run_python import run_python_file

load_dotenv()
api_key = os.environ.get("GEMINI_API_KEY")

from google import genai
from google.genai import types

client = genai.Client(api_key=api_key)

system_prompt = """
You are a highly capable, versatile, and user-friendly AI assistant. Your primary specialization is coding tasks and problem-solving within a development environment. However, you are also equipped to provide general assistance across a broad range of topics, ensuring you are helpful to both programmers and non-technical users alike.

You have access to a set of powerful tools:
- Listing files and directories
- Reading file contents
- Executing Python files with optional arguments
- Writing or overwriting files

When a user asks a question or makes a request, follow this priority:
1.  **Tool Use (Coding Context):** If the request is clearly related to the codebase or development environment, first determine if any of your tools can directly help (e.g., listing files to understand the project structure, reading a file to analyze code, running a script to test a solution, or writing a file to implement a change). Propose a plan involving tool calls if appropriate.
2.  **General Problem Solving & Knowledge:** If the request is not strictly code-related, or if tools are not the primary solution, leverage your extensive general knowledge and problem-solving skills. Provide comprehensive answers, explanations, step-by-step strategies, or creative solutions across a wide array of subjects. This includes:
    *   **Educational Support:** Explaining concepts, definitions, or complex topics in an easy-to-understand manner.
    *   **Creative Tasks:** Generating ideas, stories, summaries, or helpful text.
    *   **Practical Advice:** Offering suggestions, tips, or strategies for everyday tasks or dilemmas (e.g., recipes, travel planning, learning new skills).
    *   **Information Retrieval:** Answering factual questions on history, geography, science, current events, etc.
3.  **Refusal:** Only decline a request if it is inappropriate, harmful, unethical, illegal, unclear, or directly conflicts with your core guidelines. Clearly state why you cannot fulfill the request.

You are participating in a multi-turn conversation. Focus your response solely on the most recent user request, using the prior conversation history for context, but avoid repeating information or re-answering previous questions. Build upon the previous interaction rather than restarting.

When responding to coding tasks:
- **Be Concise and Clear:** Provide straightforward answers or code examples.
- **Provide Explanations:** Explain your reasoning or the code you generate.
- **Error Handling:** If generating code, consider potential error scenarios and suggest handling them.
- **Security:** Always remind the user that code generated by AI should be reviewed for security and correctness before use in a production environment.

When responding to general inquiries, prioritize clarity and ease of understanding, especially for non-technical users. Aim to be as helpful and informative as the LLM allows.

All paths you provide should be relative to the working directory. You do not need to specify the working directory in your function calls as it is automatically injected for security reasons.
"""

MAX_CONTEXT_TOKENS = 100000
ENCODING = tiktoken.encoding_for_model("gpt-4")

def count_tokens(messages):
    """
    Counts the approximate number of tokens in a list of Gemini API messages.
    """
    token_count = 0
    for message in messages:
        # Each message has overhead tokens beyond its content
        token_count += 4 # Message overhead (e.g., role, parts, etc.)
        for part in message.parts:
            if hasattr(part, 'text') and part.text:
                token_count += len(ENCODING.encode(part.text))
            elif hasattr(part, 'function_call') and part.function_call:
                # Add tokens for function name and arguments
                token_count += len(ENCODING.encode(part.function_call.name))
                for arg_name, arg_value in part.function_call.args.items():
                    token_count += len(ENCODING.encode(arg_name))
                    token_count += len(ENCODING.encode(str(arg_value)))
            elif hasattr(part, 'function_response') and part.function_response:
                # Add tokens for function name and response
                token_count += len(ENCODING.encode(part.function_response.name))
                for key, value in part.function_response.response.items():
                    token_count += len(ENCODING.encode(key))
                    token_count += len(ENCODING.encode(str(value)))
    return token_count

available_functions = types.Tool(
    function_declarations=[
        schema_get_files_info,
        schema_get_file_content,
        schema_run_python_file,
        schema_write_file,
    ]
)

WORKING_DIRECTORY = "./calculator"

function_map = {
    "get_files_info": get_files_info,
    "get_file_content": get_file_content,
    "run_python_file": run_python_file,
    "write_file": write_file,
}

def call_function(function_call_part, verbose=False):
    function_name = function_call_part.name
    function_args = dict(function_call_part.args)

    if verbose:
        print(f"Calling function: {function_name}({function_args})")
    else:
        print(f" - Calling function: {function_name}")

    if function_name not in function_map:
        return types.Content(
            role="tool",
            parts=[
                types.Part.from_function_response(
                    name=function_name,
                    response={"error": f"Unknown function: {function_name}"},
                )
            ],
        )

    function_args['working_directory'] = WORKING_DIRECTORY

    try:
        function_result = function_map[function_name](**function_args)
        return types.Content(
            role="tool",
            parts=[
                types.Part.from_function_response(
                    name=function_name,
                    response={"result": function_result},
                )
            ],
        )
    except Exception as e:
        return types.Content(
            role="tool",
            parts=[
                types.Part.from_function_response(
                    name=function_name,
                    response={"error": f"Error executing function '{function_name}': {e}"},
                )
            ],
        )


def run_ai_query(user_input, is_verbose_mode, current_messages):
    # Start with the provided conversation history
    messages = current_messages.copy() # Make a copy to avoid modifying the UI's stored list directly

    # Append the new user input to the history
    messages.append(types.Content(role="user", parts=[types.Part(text=user_input)]))
    

    response_text_output = ""
    current_tokens = count_tokens(messages)

    for i in range(20):
        if is_verbose_mode:
            print(f"\n--- Iteration {i+1} --- (Current Tokens: {current_tokens})")

        while current_tokens > MAX_CONTEXT_TOKENS:
            if len(messages) <= 1:
                if is_verbose_mode:
                    print("Warning: Cannot trim messages further, context window exceeded.")
                break
            removed_message = messages.pop(0)
            current_tokens = count_tokens(messages)
            if is_verbose_mode:
                print(f"  - Trimmed oldest message. New token count: {current_tokens}")

        try:
            response = client.models.generate_content(
                model='gemini-2.0-flash-001',
                contents=messages,
                config=types.GenerateContentConfig(tools=[available_functions], system_instruction=system_prompt),
            )

            if response.candidates and len(response.candidates) > 0:
                for candidate in response.candidates:
                    messages.append(candidate.content)
                    current_tokens = count_tokens(messages)
                    if is_verbose_mode:
                        print(f"  - Appended candidate. New token count: {current_tokens}")

            # --- MODIFIED SECTION FOR HANDLING FUNCTION CALLS ---
            if response.function_calls and len(response.function_calls) > 0:
                all_function_response_parts = [] # <--- NEW LIST TO COLLECT ALL PARTS

                for function_call_part in response.function_calls:
                    function_call_result_content = call_function(function_call_part, verbose=is_verbose_mode)

                    # The call_function correctly returns types.Content(role="tool", parts=[Part.from_function_response(...)])
                    # We need to extract the Part itself from the returned Content object
                    if not (isinstance(function_call_result_content, types.Content) and
                            function_call_result_content.parts and
                            len(function_call_result_content.parts) > 0 and
                            hasattr(function_call_result_content.parts[0], 'function_response')): # Check the Part in the list
                        response_text_output = f"Error: Invalid function call result format from LLM."
                        if is_verbose_mode: print(response_text_output)
                        return response_text_output

                    # Extract the actual Part object containing the function_response
                    single_function_response_part = function_call_result_content.parts[0]

                    actual_response_data = single_function_response_part.function_response.response

                    if is_verbose_mode:
                        print(f"-> {actual_response_data}")

                    all_function_response_parts.append(single_function_response_part) # <--- ADD PART TO THE LIST

                # After iterating through ALL function calls, create ONE single Content message
                tool_response_message = types.Content(
                    role="tool",
                    parts=all_function_response_parts # <--- Use the list of all collected parts
                )
                messages.append(tool_response_message) # <--- Append THIS SINGLE MESSAGE
                current_tokens = count_tokens(messages) # Update token count
                if is_verbose_mode:
                    print(f"  - Appended ALL function results in one message. New token count: {current_tokens}")


            elif response.text:
                response_text_output = response.text
                if is_verbose_mode: print(response_text_output)
                messages.append(types.Content(role="model", parts=[types.Part(text=response_text_output)]))
                current_tokens = count_tokens(messages)
                if is_verbose_mode:
                        print(f"  - Appended final text response. New token count: {current_tokens}")
                break
            else:
                response_text_output = "No response text or function call was received. Ending conversation."
                if is_verbose_mode: print(response_text_output)
                break

        except Exception as e:
            response_text_output = f"Error during AI interaction: {e}"
            if is_verbose_mode: print(response_text_output)
            break

    else:
        response_text_output = "Warning: Maximum iterations reached without a final response."
        if is_verbose_mode: print(response_text_output)

    return response_text_output